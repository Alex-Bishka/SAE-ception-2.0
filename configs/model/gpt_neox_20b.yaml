name: gpt_neox_20b
hf_path: EleutherAI/gpt-neox-20b
hidden_size: 6144
target_layer: -1  # Can also try middle layers like 16

# Training parameters
learning_rate: 1e-4
weight_decay: 0.1
warmup_steps: 500
max_grad_norm: 1.0

# Fine-tuning
epochs_per_cycle: 1
gradient_accumulation_steps: 8

# Memory optimization
use_8bit: false
use_gradient_checkpointing: true
